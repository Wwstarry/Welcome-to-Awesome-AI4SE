# üåü Welcome-to-Awesome-AI4SE
<a name="top"></a>

[![Awesome](https://awesome.re/badge.svg)](https://awesome.re) ![PRs Welcome](https://img.shields.io/badge/PRs-Welcome-brightgreen) ![Last Updated](https://img.shields.io/badge/Last%20Updated-June%2030,%202025-blue)


*I hope to maintain a curated AI4SE Paper Repository collecting recent high-quality research in software engineering. It aims to help researchers and practitioners stay updated with cutting-edge trends.*



## üóìÔ∏è 2025

- [2025-06-26] [ConTested: Consistency-Aided Tested Code Generation with LLM](https://conf.researchr.org/details/issta-2025/issta-2025-papers/27/ConTested-Consistency-Aided-Tested-Code-Generation-with-LLM)  
  *Incorporates user feedback to effectively guide consistency.*

- [2025-06-19] [Beyond PEFT: Layer-Wise Optimization for More Effective and Efficient Large Code Model Tuning](https://dl.acm.org/doi/10.1145/3729341)  
  *A comprehensive study on exploring the effectiveness of the PEFT methods.*

- [2025-05-02] [Iterative Generation of Adversarial Example for Deep Code Models](https://conf.researchr.org/details/icse-2025/icse-2025-research-track/77/Iterative-Generation-of-Adversarial-Example-for-Deep-Code-Models)  
  *A novel black-box adversarial example generation method that iteratively utilizes feedback from failed attacks to refine the generation process.*

- [2025-04-30] [Planning a Large Language Model for Static Detection of Runtime Errors in Code Snippets](https://ieeexplore.ieee.org/document/11029953)  
  *Instruct an LLM to autonomously formulate a plan to navigate through a control flow graph (CFG) for predictive execution of (in)complete code snippets.*


- [2025-04-21] [Can LLMs Replace Human Evaluators? An Empirical Study of LLM-as-a-Judge in Software Engineering](https://arxiv.org/pdf/2502.06193)  
  *Explore LLM-as-a-judge methods for evaluating SE tasks.*

- [2025-03-22] [ROCODE: Integrating Backtracking Mechanism and Program Analysis in Large Language Models for Code Generation](https://arxiv.org/pdf/2411.07112)  
  *Integrates backtracking mechanism and program analysis tools.*

- [2025-03-18] [HumanEvo: An Evolution-aware Benchmark for More Realistic Evaluation of Repository-level Code Generation](https://arxiv.org/pdf/2406.06918)  
  *Construct an evolution-aware repository-level code generation dataset.*

- [2025-02-13] [Knowledge-Enhanced Program Repair for Data Science Code](https://arxiv.org/pdf/2502.09771)  
  *A knowledge-enhanced program repair approach designed to repair the buggy code generated by LLMs in the data science domain.*

- [2025-02-05] [COFFE: A Code Efficiency Benchmark for Code Generation](https://arxiv.org/pdf/2502.02827)  
  *A new benchmark COFFE for the time efficiency evaluation of LLMgenerated code.*

- [2025-02-01] [Patch Synthesis for Property Repair of Deep Neural Networks](https://arxiv.org/pdf/2404.01642)  
  *Incorporate formal verification and a heuristic mechanism for allocating patch modules.*

- [2025-01-04] [CoreCodeBench: A Configurable Multi-Scenario Repository-Level Benchmark](https://arxiv.org/pdf/2507.05281)  
  *Provide a comprehensive and extensive repository-level benchmark to investigate the applicability of LLMs in real-world engineering projects.*






## üóìÔ∏è 2024


- [2024-11-12] [Qwen2.5-Coder Technical Report](https://arxiv.org/pdf/2409.12186)  
  *Introduce the Qwen2.5-Coder series.*

- [2024-09-21] [Reasoning Runtime Behavior of a Program with LLM: How Far Are We?](https://arxiv.org/pdf/2403.16437)  
  *Evaluate the code reasoning capability of code LLMs.*

- [2024-08-22] [Search-Based LLMs for Code Optimization](https://arxiv.org/pdf/2408.12159)  
  *Integrate LLMs with evolutionary search.*

- [2024-07-28] [RLCoder: Reinforcement Learning for Repository-Level Code Completion](https://arxiv.org/pdf/2407.19487)  
  *A novel reinforcement learning framework for repository-level code completion.*

- [2024-02-23] [CoderEval: A Benchmark of Pragmatic Code Generation with Generative Pre-trained Models](https://arxiv.org/pdf/2507.05281)  
  *A new benchmark to evaluate a model‚Äôs effectiveness in pragmatic code generation scenarios.*

- [2024-01-30] [Between Lines of Code: Unraveling the Distinct Patterns of Machine and Human Programmers](https://arxiv.org/pdf/2401.06461)  
  *Study the specific patterns that characterize machine- and human-authored code.*


## üöÄ Useful GitHub Repos & Models

Here are some great open-source resources and models related to **AI4SE** that are worth checking out:

-  **[SWE-bench](https://github.com/princeton-nlp/SWE-bench)**  
  A benchmark for evaluating LLMs on real-world GitHub issue resolution.  




## üì¨ Contact

üí° **Working on something exciting in AI4SE?** I'd love to hear from you ‚Äî feel free to get in touch: panruwei@stu.cqu.edu.cn

üå± I'm currently exploring internship opportunities where I can contribute to real-world applications of AI for Software Engineering. Let's connect!


